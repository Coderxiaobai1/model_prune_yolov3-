{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils.utils import *\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from test import test\n",
    "from terminaltables import AsciiTable\n",
    "import time\n",
    "from utils.prune_utils import *\n",
    "import argparse\n",
    "\n",
    "#0.419     0.473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--cfg', type=str, default='cfg/dense_yolov3_4.cfg', help='cfg file path')\n",
    "parser.add_argument('--data', type=str, default='data/visdrone.data', help='*.data file path')\n",
    "parser.add_argument('--weights', type=str, default='weights/best.pt', help='sparse model weights')\n",
    "parser.add_argument('--global_percent', type=float, default=0.8, help='global channel prune percent')\n",
    "parser.add_argument('--layer_keep', type=float, default=0.01, help='channel keep percent per layer')\n",
    "parser.add_argument('--img_size', type=int, default=800, help='inference size (pixels)')\n",
    "opt = parser.parse_known_args()[0]\n",
    "opt.cfg = check_file(opt.cfg)  # check file\n",
    "opt.data = check_file(opt.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 254 layers, 6.19432e+07 parameters, 6.19432e+07 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching labels /home/tione/notebook/seu/VisDrone2019-DET-val/labels.txt (548 found, 0 missing, 0 empty, 0 duplicate, for 548 images): 100%|██████████| 548/548 [00:00<00:00, 3346.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded weights from  weights/best.pt\n",
      "\n",
      "let's test the original model first:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "               Class    Images   Targets         P         R   mAP@0.5        F1:   0%|          | 0/9 [00:00<?, ?it/s]/home/tione/notebook/seu/yolov3-master/utils/utils.py:512: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  i, j = (x[:, 5:] > conf_thres).nonzero().t()\n",
      "               Class    Images   Targets         P         R   mAP@0.5        F1: 100%|██████████| 9/9 [01:48<00:00, 12.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       548  3.88e+04     0.398     0.484     0.422     0.435\n"
     ]
    }
   ],
   "source": [
    "img_size = opt.img_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Darknet(opt.cfg, (img_size, img_size)).to(device)\n",
    "\n",
    "if opt.weights.endswith(\".pt\"):\n",
    "    model.load_state_dict(torch.load(opt.weights, map_location=device)['model'])\n",
    "else:\n",
    "    _ = load_darknet_weights(model, opt.weights)\n",
    "print('\\nloaded weights from ',opt.weights)\n",
    "\n",
    "eval_model = lambda model:test(model = model, cfg=opt.cfg, data=opt.data,batch_size=64, imgsz=img_size,is_training = False)\n",
    "obtain_num_parameters = lambda model:sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(\"\\nlet's test the original model first:\")\n",
    "with torch.no_grad():\n",
    "    origin_model_metric = eval_model(model)\n",
    "origin_nparameters = obtain_num_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Threshold should be less than 0.0000.\n"
     ]
    }
   ],
   "source": [
    "CBL_idx, Conv_idx, prune_idx, _, _= parse_module_defs2(model.module_defs)\n",
    "bn_weights = gather_bn_weights(model.module_list, prune_idx)\n",
    "\n",
    "sorted_bn = torch.sort(bn_weights)[0]\n",
    "sorted_bn, sorted_index = torch.sort(bn_weights)\n",
    "thresh_index = int(len(bn_weights) * opt.global_percent)\n",
    "thresh = sorted_bn[thresh_index].cuda()\n",
    "\n",
    "print(f'Global Threshold should be less than {thresh:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer index:   0 \t total channel:   32 \t remaining channel:   29\n",
      "layer index:   1 \t total channel:   64 \t remaining channel:   55\n",
      "layer index:   2 \t total channel:   32 \t remaining channel:   28\n",
      "layer index:   3 \t total channel:   64 \t remaining channel:   39\n",
      "layer index:   5 \t total channel:  128 \t remaining channel:  119\n",
      "layer index:   6 \t total channel:   64 \t remaining channel:   32\n",
      "layer index:   7 \t total channel:  128 \t remaining channel:   77\n",
      "layer index:   9 \t total channel:   64 \t remaining channel:   52\n",
      "layer index:  10 \t total channel:  128 \t remaining channel:   84\n",
      "layer index:  12 \t total channel:  256 \t remaining channel:  192\n",
      "layer index:  13 \t total channel:  128 \t remaining channel:   38\n",
      "layer index:  14 \t total channel:  256 \t remaining channel:   14\n",
      "layer index:  16 \t total channel:  128 \t remaining channel:    6\n",
      "layer index:  17 \t total channel:  256 \t remaining channel:   19\n",
      "layer index:  19 \t total channel:  128 \t remaining channel:    5\n",
      "layer index:  20 \t total channel:  256 \t remaining channel:    8\n",
      "layer index:  22 \t total channel:  128 \t remaining channel:   22\n",
      "layer index:  23 \t total channel:  256 \t remaining channel:   31\n",
      "layer index:  25 \t total channel:  128 \t remaining channel:   18\n",
      "layer index:  26 \t total channel:  256 \t remaining channel:   29\n",
      "layer index:  28 \t total channel:  128 \t remaining channel:   34\n",
      "layer index:  29 \t total channel:  256 \t remaining channel:   49\n",
      "layer index:  31 \t total channel:  128 \t remaining channel:   32\n",
      "layer index:  32 \t total channel:  256 \t remaining channel:   56\n",
      "layer index:  34 \t total channel:  128 \t remaining channel:   67\n",
      "layer index:  35 \t total channel:  256 \t remaining channel:   73\n",
      "layer index:  37 \t total channel:  512 \t remaining channel:   80\n",
      "layer index:  38 \t total channel:  256 \t remaining channel:  142\n",
      "layer index:  39 \t total channel:  512 \t remaining channel:   37\n",
      "layer index:  41 \t total channel:  256 \t remaining channel:    6\n",
      "layer index:  42 \t total channel:  512 \t remaining channel:   38\n",
      "layer index:  44 \t total channel:  256 \t remaining channel:  142\n",
      "layer index:  45 \t total channel:  512 \t remaining channel:   22\n",
      "layer index:  47 \t total channel:  256 \t remaining channel:  159\n",
      "layer index:  48 \t total channel:  512 \t remaining channel:    9\n",
      "layer index:  50 \t total channel:  256 \t remaining channel:  158\n",
      "layer index:  51 \t total channel:  512 \t remaining channel:   22\n",
      "layer index:  53 \t total channel:  256 \t remaining channel:  158\n",
      "layer index:  54 \t total channel:  512 \t remaining channel:   12\n",
      "layer index:  56 \t total channel:  256 \t remaining channel:   10\n",
      "layer index:  57 \t total channel:  512 \t remaining channel:   45\n",
      "layer index:  59 \t total channel:  256 \t remaining channel:   12\n",
      "layer index:  60 \t total channel:  512 \t remaining channel:   27\n",
      "layer index:  62 \t total channel: 1024 \t remaining channel:   22\n",
      "layer index:  63 \t total channel:  512 \t remaining channel:  302\n",
      "layer index:  64 \t total channel: 1024 \t remaining channel:   30\n",
      "layer index:  66 \t total channel:  512 \t remaining channel:  283\n",
      "layer index:  67 \t total channel: 1024 \t remaining channel:   35\n",
      "layer index:  69 \t total channel:  512 \t remaining channel:   94\n",
      "layer index:  70 \t total channel: 1024 \t remaining channel:   51\n",
      "layer index:  72 \t total channel:  512 \t remaining channel:  320\n",
      "layer index:  73 \t total channel: 1024 \t remaining channel:   23\n",
      "layer index:  75 \t total channel:  512 \t remaining channel:   46\n",
      "layer index:  76 \t total channel: 1024 \t remaining channel:   58\n",
      "layer index:  77 \t total channel:  512 \t remaining channel:   49\n",
      "layer index:  78 \t total channel: 1024 \t remaining channel:   69\n",
      "layer index:  79 \t total channel:  512 \t remaining channel:   55\n",
      "layer index:  80 \t total channel: 1024 \t remaining channel:  131\n",
      "layer index:  87 \t total channel:  256 \t remaining channel:   76\n",
      "layer index:  88 \t total channel:  512 \t remaining channel:   97\n",
      "layer index:  89 \t total channel:  256 \t remaining channel:   77\n",
      "layer index:  90 \t total channel:  512 \t remaining channel:  101\n",
      "layer index:  91 \t total channel:  256 \t remaining channel:   92\n",
      "layer index:  92 \t total channel:  512 \t remaining channel:  140\n",
      "layer index: 102 \t total channel:  128 \t remaining channel:   74\n",
      "layer index: 103 \t total channel:  256 \t remaining channel:  104\n",
      "layer index: 104 \t total channel:  128 \t remaining channel:   68\n",
      "layer index: 105 \t total channel:  256 \t remaining channel:   95\n",
      "layer index: 106 \t total channel:  128 \t remaining channel:   65\n",
      "layer index: 107 \t total channel:  256 \t remaining channel:   93\n",
      "layer index: 120 \t total channel:   64 \t remaining channel:   60\n",
      "layer index: 121 \t total channel:  128 \t remaining channel:   97\n",
      "layer index: 122 \t total channel:   64 \t remaining channel:   51\n",
      "layer index: 123 \t total channel:  128 \t remaining channel:   74\n",
      "layer index: 124 \t total channel:   64 \t remaining channel:   31\n",
      "layer index: 125 \t total channel:  128 \t remaining channel:   49\n",
      "Prune channels: 21197\tPrune ratio: 0.782\n"
     ]
    }
   ],
   "source": [
    "def obtain_filters_mask(model, thre, CBL_idx, prune_idx):\n",
    "\n",
    "    pruned = 0\n",
    "    total = 0\n",
    "    num_filters = []\n",
    "    filters_mask = []\n",
    "    for idx in CBL_idx:\n",
    "        bn_module = model.module_list[idx][1]\n",
    "        if idx in prune_idx:\n",
    "\n",
    "            weight_copy = bn_module.weight.data.abs().clone()\n",
    "\n",
    "            channels = weight_copy.shape[0] #\n",
    "            min_channel_num = int(channels * opt.layer_keep) if int(channels * opt.layer_keep) > 0 else 1\n",
    "            mask = weight_copy.gt(thresh).float()\n",
    "\n",
    "            if int(torch.sum(mask)) < min_channel_num: \n",
    "                _, sorted_index_weights = torch.sort(weight_copy,descending=True)\n",
    "                mask[sorted_index_weights[:min_channel_num]]=1. \n",
    "            remain = int(mask.sum())\n",
    "            pruned = pruned + mask.shape[0] - remain\n",
    "\n",
    "            print(f'layer index: {idx:>3d} \\t total channel: {mask.shape[0]:>4d} \\t '\n",
    "                    f'remaining channel: {remain:>4d}')\n",
    "        else:\n",
    "            mask = torch.ones(bn_module.weight.data.shape)\n",
    "            remain = mask.shape[0]\n",
    "\n",
    "        total += mask.shape[0]\n",
    "        num_filters.append(remain)\n",
    "        filters_mask.append(mask.clone())\n",
    "\n",
    "    prune_ratio = pruned / total\n",
    "    print(f'Prune channels: {pruned}\\tPrune ratio: {prune_ratio:.3f}')\n",
    "\n",
    "    return num_filters, filters_mask\n",
    "\n",
    "num_filters, filters_mask = obtain_filters_mask(model, thresh, CBL_idx, prune_idx)\n",
    "CBLidx2mask = {idx: mask for idx, mask in zip(CBL_idx, filters_mask)}\n",
    "CBLidx2filters = {idx: filters for idx, filters in zip(CBL_idx, num_filters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge the mask of layers connected to shortcut!\n"
     ]
    }
   ],
   "source": [
    "for i in model.module_defs:\n",
    "    if i['type'] == 'shortcut':\n",
    "        i['is_access'] = False\n",
    "\n",
    "print('merge the mask of layers connected to shortcut!')\n",
    "merge_mask(model, CBLidx2mask, CBLidx2filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "now prune the model but keep size,(actually add offset of BN beta to following layers), let's see how the mAP goes\n"
     ]
    }
   ],
   "source": [
    "for i in CBLidx2mask:\n",
    "    CBLidx2mask[i] = CBLidx2mask[i].clone().cpu().numpy()\n",
    "\n",
    "pruned_model = prune_model_keep_size2(model, prune_idx, CBL_idx, CBLidx2mask)\n",
    "print(\"\\nnow prune the model but keep size,(actually add offset of BN beta to following layers), let's see how the mAP goes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def prune_and_eval(model, CBL_idx, CBLidx2mask):\n",
    "#     model_copy = deepcopy(model)\n",
    "\n",
    "#     for idx in CBL_idx:\n",
    "#         bn_module = model_copy.module_list[idx][1]\n",
    "#         mask = CBLidx2mask[idx].cuda()\n",
    "#         bn_module.weight.data.mul_(mask)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         mAP = eval_model(model_copy)[0][2]\n",
    "\n",
    "#     print(f'mask the gamma as zero, mAP of the model is {mAP:.4f}')\n",
    "\n",
    "\n",
    "# prune_and_eval(model, CBL_idx, CBLidx2mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 254 layers, 4.9728e+06 parameters, 4.9728e+06 gradients\n"
     ]
    }
   ],
   "source": [
    "for i in model.module_defs:\n",
    "    if i['type'] == 'shortcut':\n",
    "        i.pop('is_access')\n",
    "\n",
    "compact_module_defs = deepcopy(model.module_defs)\n",
    "for idx in CBL_idx:\n",
    "    assert compact_module_defs[idx]['type'] == 'convolutional'\n",
    "    compact_module_defs[idx]['filters'] = str(CBLidx2filters[idx])\n",
    "\n",
    "\n",
    "compact_model = Darknet([model.hyperparams.copy()] + compact_module_defs, (img_size, img_size)).to(device)\n",
    "compact_nparameters = obtain_num_parameters(compact_model)\n",
    "\n",
    "init_weights_from_loose_model(compact_model, pruned_model, CBL_idx, Conv_idx, CBLidx2mask)\n",
    "\n",
    "\n",
    "random_input = torch.rand((1, 3, img_size, img_size)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing inference time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching labels /home/tione/notebook/seu/VisDrone2019-DET-val/labels.txt (548 found, 0 missing, 0 empty, 0 duplicate, for 548 images): 100%|██████████| 548/548 [00:00<00:00, 3342.33it/s]\n",
      "               Class    Images   Targets         P         R   mAP@0.5        F1:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               Class    Images   Targets         P         R   mAP@0.5        F1: 100%|██████████| 9/9 [01:45<00:00, 11.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       548  3.88e+04     0.398     0.484     0.421     0.435\n",
      "+------------+----------+----------+\n",
      "| Metric     | Before   | After    |\n",
      "+------------+----------+----------+\n",
      "| mAP        | 0.421684 | 0.420999 |\n",
      "| Parameters | 61943156 | 4972797  |\n",
      "| Inference  | 0.0230   | 0.0125   |\n",
      "+------------+----------+----------+\n",
      "Config file has been saved: cfg/prune_0.8_keep_0.01_dense_yolov3_4.cfg\n",
      "Compact model has been saved: weights/prune_0.8_keep_0.01_best.weights\n"
     ]
    }
   ],
   "source": [
    "def obtain_avg_forward_time(input, model, repeat=200):\n",
    "\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(repeat):\n",
    "            output = model(input)\n",
    "    avg_infer_time = (time.time() - start) / repeat\n",
    "\n",
    "    return avg_infer_time, output\n",
    "\n",
    "print('testing inference time...')\n",
    "pruned_forward_time, pruned_output = obtain_avg_forward_time(random_input, pruned_model)\n",
    "compact_forward_time, compact_output = obtain_avg_forward_time(random_input, compact_model)\n",
    "\n",
    "print('testing the final model...')\n",
    "with torch.no_grad():\n",
    "    compact_model_metric = eval_model(compact_model)\n",
    "    \n",
    "metric_table = [\n",
    "\n",
    "    [\"Metric\", \"Before\", \"After\"],\n",
    "    [\"mAP\", f'{origin_model_metric[0][2]:.6f}', f'{compact_model_metric[0][2]:.6f}'],\n",
    "    [\"Parameters\", f\"{origin_nparameters}\", f\"{compact_nparameters}\"],\n",
    "    [\"Inference\", f'{pruned_forward_time:.4f}', f'{compact_forward_time:.4f}']\n",
    "]\n",
    "print(AsciiTable(metric_table).table)\n",
    "\n",
    "\n",
    "\n",
    "pruned_cfg_name = opt.cfg.replace('/', f'/prune_{opt.global_percent}_keep_{opt.layer_keep}_')\n",
    "pruned_cfg_file = write_cfg(pruned_cfg_name, [model.hyperparams.copy()] + compact_module_defs)\n",
    "print(f'Config file has been saved: {pruned_cfg_file}')\n",
    "\n",
    "compact_model_name = opt.weights.replace('/', f'/prune_{opt.global_percent}_keep_{opt.layer_keep}_')\n",
    "if compact_model_name.endswith('.pt'):\n",
    "    compact_model_name = compact_model_name.replace('.pt', '.weights')\n",
    "save_weights(compact_model, path=compact_model_name)\n",
    "print(f'Compact model has been saved: {compact_model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[600, 800, 800]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = [600,800]\n",
    "img_size.extend([img_size[-1]] * (3 - len(img_size)))\n",
    "img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Sequential(\n",
       "    (Conv2d): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (Conv2d): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (Conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (4): WeightedFeatureFusion()\n",
       "  (5): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (8): WeightedFeatureFusion()\n",
       "  (9): Sequential(\n",
       "    (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (10): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (11): WeightedFeatureFusion()\n",
       "  (12): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (13): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (14): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (15): WeightedFeatureFusion()\n",
       "  (16): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (17): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (18): WeightedFeatureFusion()\n",
       "  (19): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (20): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (21): WeightedFeatureFusion()\n",
       "  (22): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (23): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (24): WeightedFeatureFusion()\n",
       "  (25): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (26): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (27): WeightedFeatureFusion()\n",
       "  (28): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (29): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (30): WeightedFeatureFusion()\n",
       "  (31): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (32): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (33): WeightedFeatureFusion()\n",
       "  (34): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (35): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (36): WeightedFeatureFusion()\n",
       "  (37): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (38): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (39): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (40): WeightedFeatureFusion()\n",
       "  (41): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (42): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (43): WeightedFeatureFusion()\n",
       "  (44): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (45): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (46): WeightedFeatureFusion()\n",
       "  (47): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (48): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (49): WeightedFeatureFusion()\n",
       "  (50): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (51): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (52): WeightedFeatureFusion()\n",
       "  (53): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (54): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (55): WeightedFeatureFusion()\n",
       "  (56): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (57): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (58): WeightedFeatureFusion()\n",
       "  (59): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (60): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (61): WeightedFeatureFusion()\n",
       "  (62): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (63): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (64): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (65): WeightedFeatureFusion()\n",
       "  (66): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (67): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (68): WeightedFeatureFusion()\n",
       "  (69): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (70): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (71): WeightedFeatureFusion()\n",
       "  (72): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (73): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (74): WeightedFeatureFusion()\n",
       "  (75): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (76): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (77): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (78): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (79): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (80): Sequential(\n",
       "    (Conv2d): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(1024, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (81): Sequential(\n",
       "    (Conv2d): Conv2d(1024, 45, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (82): YOLOLayer()\n",
       "  (83): FeatureConcat()\n",
       "  (84): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (85): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (86): FeatureConcat()\n",
       "  (87): Sequential(\n",
       "    (Conv2d): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (88): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (89): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (90): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (91): Sequential(\n",
       "    (Conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (92): Sequential(\n",
       "    (Conv2d): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(512, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (93): Sequential(\n",
       "    (Conv2d): Conv2d(512, 45, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (94): YOLOLayer()\n",
       "  (95): FeatureConcat()\n",
       "  (96): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (97): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (98): FeatureConcat()\n",
       "  (99): Sequential(\n",
       "    (Conv2d): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (100): Upsample(scale_factor=4.0, mode=nearest)\n",
       "  (101): FeatureConcat()\n",
       "  (102): Sequential(\n",
       "    (Conv2d): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (103): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (104): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (105): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (106): Sequential(\n",
       "    (Conv2d): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (107): Sequential(\n",
       "    (Conv2d): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(256, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (108): Sequential(\n",
       "    (Conv2d): Conv2d(256, 45, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (109): YOLOLayer()\n",
       "  (110): FeatureConcat()\n",
       "  (111): Sequential(\n",
       "    (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (112): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (113): FeatureConcat()\n",
       "  (114): Sequential(\n",
       "    (Conv2d): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(16, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (115): Upsample(scale_factor=4.0, mode=nearest)\n",
       "  (116): FeatureConcat()\n",
       "  (117): Sequential(\n",
       "    (Conv2d): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (118): Upsample(scale_factor=8.0, mode=nearest)\n",
       "  (119): FeatureConcat()\n",
       "  (120): Sequential(\n",
       "    (Conv2d): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (121): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (122): Sequential(\n",
       "    (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (123): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (124): Sequential(\n",
       "    (Conv2d): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(64, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (125): Sequential(\n",
       "    (Conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  )\n",
       "  (126): Sequential(\n",
       "    (Conv2d): Conv2d(128, 45, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (127): YOLOLayer()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model.module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 32,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 64,\n",
       "  'size': 3,\n",
       "  'stride': 2,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 32,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 64,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 3,\n",
       "  'stride': 2,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 64,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 64,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 3,\n",
       "  'stride': 2,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 3,\n",
       "  'stride': 2,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 1024,\n",
       "  'size': 3,\n",
       "  'stride': 2,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 1024,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 1024,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 1024,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 1024,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'shortcut', 'from': [-3], 'activation': 'linear', 'is_access': True},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 1024,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 1024,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 512,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 1024,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 0,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 45,\n",
       "  'activation': 'linear'},\n",
       " {'type': 'yolo',\n",
       "  'mask': [9, 10, 11],\n",
       "  'anchors': array([[          5,          10],\n",
       "         [          9,          23],\n",
       "         [         17,          41],\n",
       "         [         37,          32],\n",
       "         [         30,          61],\n",
       "         [         62,          45],\n",
       "         [         59,         119],\n",
       "         [         97,         134],\n",
       "         [         65,          65],\n",
       "         [        116,          90],\n",
       "         [        156,         198],\n",
       "         [        373,         326]]),\n",
       "  'classes': 10,\n",
       "  'num': 12,\n",
       "  'jitter': '.3',\n",
       "  'ignore_thresh': '.7',\n",
       "  'truth_thresh': 1,\n",
       "  'random': 1},\n",
       " {'type': 'route', 'layers': [-4]},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'upsample', 'stride': 2},\n",
       " {'type': 'route', 'layers': [-1, 61]},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 512,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 512,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 256,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 512,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 0,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 45,\n",
       "  'activation': 'linear'},\n",
       " {'type': 'yolo',\n",
       "  'mask': [6, 7, 8],\n",
       "  'anchors': array([[          5,          10],\n",
       "         [          9,          23],\n",
       "         [         17,          41],\n",
       "         [         37,          32],\n",
       "         [         30,          61],\n",
       "         [         62,          45],\n",
       "         [         59,         119],\n",
       "         [         97,         134],\n",
       "         [         65,          65],\n",
       "         [        116,          90],\n",
       "         [        156,         198],\n",
       "         [        373,         326]]),\n",
       "  'classes': 10,\n",
       "  'num': 12,\n",
       "  'jitter': '.3',\n",
       "  'ignore_thresh': '.7',\n",
       "  'truth_thresh': 1,\n",
       "  'random': 1},\n",
       " {'type': 'route', 'layers': [-4]},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'upsample', 'stride': 2},\n",
       " {'type': 'route', 'layers': [79]},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'upsample', 'stride': 4},\n",
       " {'type': 'route', 'layers': [-1, -4, 36]},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 256,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 256,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 128,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 256,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 0,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 45,\n",
       "  'activation': 'linear'},\n",
       " {'type': 'yolo',\n",
       "  'mask': [3, 4, 5],\n",
       "  'anchors': array([[          5,          10],\n",
       "         [          9,          23],\n",
       "         [         17,          41],\n",
       "         [         37,          32],\n",
       "         [         30,          61],\n",
       "         [         62,          45],\n",
       "         [         59,         119],\n",
       "         [         97,         134],\n",
       "         [         65,          65],\n",
       "         [        116,          90],\n",
       "         [        156,         198],\n",
       "         [        373,         320]]),\n",
       "  'classes': 10,\n",
       "  'num': 12,\n",
       "  'jitter': '.3',\n",
       "  'ignore_thresh': '.7',\n",
       "  'truth_thresh': 1,\n",
       "  'random': 1},\n",
       " {'type': 'route', 'layers': [-4]},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 64,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'upsample', 'stride': 2},\n",
       " {'type': 'route', 'layers': [91]},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 16,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'upsample', 'stride': 4},\n",
       " {'type': 'route', 'layers': [79]},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 32,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'upsample', 'stride': 8},\n",
       " {'type': 'route', 'layers': [-1, -4, -7, 11]},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 64,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 128,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 64,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 128,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'filters': 64,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 1,\n",
       "  'size': 3,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 128,\n",
       "  'activation': 'leaky'},\n",
       " {'type': 'convolutional',\n",
       "  'batch_normalize': 0,\n",
       "  'size': 1,\n",
       "  'stride': 1,\n",
       "  'pad': 1,\n",
       "  'filters': 45,\n",
       "  'activation': 'linear'},\n",
       " {'type': 'yolo',\n",
       "  'mask': [0, 1, 2],\n",
       "  'anchors': array([[          5,          10],\n",
       "         [          9,          23],\n",
       "         [         17,          41],\n",
       "         [         37,          32],\n",
       "         [         30,          61],\n",
       "         [         62,          45],\n",
       "         [         59,         119],\n",
       "         [         97,         134],\n",
       "         [         65,          65],\n",
       "         [        116,          90],\n",
       "         [        156,         198],\n",
       "         [        373,         326]]),\n",
       "  'classes': 10,\n",
       "  'num': 12,\n",
       "  'jitter': '.3',\n",
       "  'ignore_thresh': '.7',\n",
       "  'truth_thresh': 1,\n",
       "  'random': 1}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model.module_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 4,  5,  6],\n",
       "       [11, 21, 31],\n",
       "       [ 7,  8,  9]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([[1,2,3],[4,5,6]])\n",
    "b=np.array([[11,21,31],[7,8,9]])\n",
    "np.concatenate([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "i =12\n",
    "\n",
    "if i ==1:\n",
    "    print('1')\n",
    "elif i ==2:\n",
    "    print('2')\n",
    "else:\n",
    "    print('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                 1,           1,           1,           1,           1,           0,           1,           1], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBLidx2mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                 1,           1,           1,           1,           1,           0,           1,           1,           1,           0,           1,           1,           1,           1,           1,           1,           1,           0,           1,           1,           1,           1,           1,           1,\n",
       "                 1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                 1,           1,           1,           1,           1,           0,           1,           1,           1,           1,           1,           1,           1,           1,           1,           0,           1,           1,           1,           0,           1,           1,           1,           1,\n",
       "                 1,           1,           1,           1,           1,           0,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                 1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                 1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                 1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                 1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
       "                 1,           1,           1,           1,           1,           1,           1,           1], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([CBLidx2mask[0],CBLidx2mask[1],CBLidx2mask[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
