{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-32GB', total_memory=32480MB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import argparse\n",
    "\n",
    "from models import *  # set ONNX_EXPORT in models.py\n",
    "from utils.datasets import *\n",
    "from utils.utils import *\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--cfg', type=str, default='./cfg/dense_yolov3_4.cfg', help='*.cfg path')\n",
    "parser.add_argument('--img_path', type=str, default='./data/car/', help='*.img path')\n",
    "parser.add_argument('--names', type=str, default='./data/visdrone.names', help='*.names path')\n",
    "parser.add_argument('--weights', type=str, default='./weights/dense_yolov3_4_last_1024.pt', help='weights path')\n",
    "parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
    "parser.add_argument('--conf-thres', type=float, default=0.3, help='object confidence threshold')\n",
    "parser.add_argument('--iou-thres', type=float, default=0.6, help='IOU threshold for NMS')\n",
    "parser.add_argument('--fourcc', type=str, default='mp4v', help='output video codec (verify ffmpeg support)')\n",
    "parser.add_argument('--device', default='0', help='device id (i.e. 0 or 0,1) or cpu')\n",
    "parser.add_argument('--classes', nargs='+', type=int, help='filter by class')\n",
    "parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
    "opt = parser.parse_known_args()[0]\n",
    "\n",
    "device = torch_utils.select_device(device=opt.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_pred(object):\n",
    "    def __init__(self, opt=opt, device=device):\n",
    "        super(model_pred, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.device = device\n",
    "\n",
    "    def load_model(self, weight_path, cfg_path, imgsz):\n",
    "        model = Darknet(cfg_path, imgsz)\n",
    "        model.load_state_dict(torch.load(weight_path, map_location=self.device)['model'])\n",
    "        model.to(self.device).eval()\n",
    "        return model\n",
    "\n",
    "    def get_pred(self, img, model):\n",
    "        with torch.no_grad():\n",
    "            pred = model(img, augment=False)[0]\n",
    "            pred = non_max_suppression(pred, self.opt.conf_thres, self.opt.iou_thres,\n",
    "                                       multi_label=False, classes=self.opt.classes, agnostic=self.opt.agnostic_nms)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def img_process(self, img_dir_update):\n",
    "        # 对图像的处理\n",
    "        img0 = cv2.imread(img_dir_update)\n",
    "        img = letterbox(img0, new_shape=640)[0]\n",
    "        # Convert\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        img = torch.from_numpy(img).to(self.device)  # to GPU\n",
    "        img = img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        return img, img0\n",
    "\n",
    "\n",
    "class Inference(model_pred):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Inference, self).__init__()\n",
    "        self.img_dir = self.opt.img_path\n",
    "        self.weight_dir = self.opt.weights\n",
    "        self.cfg_dir = self.opt.cfg\n",
    "\n",
    "        self.model = self.load_model(self.weight_dir, self.cfg_dir, self.opt.img_size)\n",
    "\n",
    "    def infer_toresult(self):\n",
    "        path = self.img_dir\n",
    "        file_path = os.listdir(path)\n",
    "        file_path.sort(key = lambda x:int(x[:-4]))\n",
    "        for img_path in file_path:\n",
    "            name=img_path[:-4]\n",
    "            img, img0 = self.img_process(os.path.join(path, img_path))\n",
    "            t1 = torch_utils.time_synchronized()\n",
    "            pred = self.get_pred(img, self.model)\n",
    "            t2 = torch_utils.time_synchronized()\n",
    "            print('inference time:',t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 254 layers, 6.19432e+07 parameters, 6.19432e+07 gradients\n",
      "inference time: 0.472398042678833\n",
      "inference time: 0.014076709747314453\n",
      "inference time: 0.014065027236938477\n",
      "inference time: 0.01400446891784668\n",
      "inference time: 0.01405191421508789\n",
      "inference time: 0.014070749282836914\n",
      "inference time: 0.014123678207397461\n",
      "inference time: 0.013996601104736328\n",
      "inference time: 0.013956069946289062\n",
      "inference time: 0.013988018035888672\n",
      "inference time: 0.014059782028198242\n",
      "inference time: 0.013944625854492188\n",
      "inference time: 0.014040708541870117\n",
      "inference time: 0.014110565185546875\n",
      "inference time: 0.01398468017578125\n",
      "inference time: 0.013993978500366211\n",
      "inference time: 0.014000654220581055\n",
      "inference time: 0.014089345932006836\n",
      "inference time: 0.014046669006347656\n",
      "inference time: 0.01400899887084961\n",
      "inference time: 0.013972997665405273\n",
      "inference time: 0.014053821563720703\n",
      "inference time: 0.014127254486083984\n",
      "inference time: 0.014037132263183594\n",
      "inference time: 0.014081954956054688\n",
      "inference time: 0.014118194580078125\n",
      "inference time: 0.014024972915649414\n",
      "inference time: 0.01407623291015625\n",
      "inference time: 0.013985633850097656\n",
      "inference time: 0.014036178588867188\n",
      "inference time: 0.014042139053344727\n",
      "inference time: 0.014030218124389648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f41fe90b4711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minfer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_toresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-4cca2893f839>\u001b[0m in \u001b[0;36minfer_toresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_synchronized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4cca2893f839>\u001b[0m in \u001b[0;36mimg_process\u001b[0;34m(self, img_dir_update)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimg_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dir_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# 对图像的处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mimg0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mletterbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Convert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    infer = Inference()\n",
    "    infer.infer_toresult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tione/notebook/seu/yolov3-master'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
