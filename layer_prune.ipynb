{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils.utils import *\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from test import test\n",
    "from terminaltables import AsciiTable\n",
    "import time\n",
    "from utils.prune_utils import *\n",
    "import argparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--cfg', type=str, default='cfg/prune_0.8_keep_0.01_dense_yolov3_4.cfg', help='cfg file path')\n",
    "parser.add_argument('--data', type=str, default='data/visdrone.data', help='*.data file path')\n",
    "parser.add_argument('--weights', type=str, default='weights/prune_0.8_keep_0.01_best.weights', help='sparse model weights')\n",
    "parser.add_argument('--shortcuts', type=int, default=12, help='how many shortcut layers will be pruned,\\\n",
    "    pruning one shortcut will also prune two CBL,yolov3 has 23 shortcuts')\n",
    "parser.add_argument('--img_size', type=int, default=800, help='inference size (pixels)')\n",
    "opt = parser.parse_known_args()[0]\n",
    "opt.cfg = check_file(opt.cfg)  # check file\n",
    "opt.data = check_file(opt.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 254 layers, 4.9728e+06 parameters, 4.9728e+06 gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching labels /home/tione/notebook/seu/VisDrone2019-DET-val/labels.txt (548 found, 0 missing, 0 empty, 0 duplicate, for 548 images): 100%|██████████| 548/548 [00:00<00:00, 3343.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded weights from  weights/prune_0.8_keep_0.01_best.weights\n",
      "\n",
      "let's test the original model first:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "               Class    Images   Targets         P         R   mAP@0.5        F1: 100%|██████████| 9/9 [01:44<00:00, 11.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       548  3.88e+04     0.398     0.484     0.421     0.435\n"
     ]
    }
   ],
   "source": [
    "img_size = opt.img_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Darknet(opt.cfg, (img_size, img_size)).to(device)\n",
    "\n",
    "if opt.weights.endswith(\".pt\"):\n",
    "    model.load_state_dict(torch.load(opt.weights, map_location=device)['model'])\n",
    "else:\n",
    "    _ = load_darknet_weights(model, opt.weights)\n",
    "print('\\nloaded weights from ',opt.weights)\n",
    "\n",
    "eval_model = lambda model:test(model = model, cfg=opt.cfg, data=opt.data,batch_size=64, imgsz=img_size,is_training = False)\n",
    "obtain_num_parameters = lambda model:sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(\"\\nlet's test the original model first:\")\n",
    "with torch.no_grad():\n",
    "    origin_model_metric = eval_model(model)\n",
    "origin_nparameters = obtain_num_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_defs = {idx: mask for idx, mask in enumerate(model.module_defs)}\n",
    "# model_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These shortcut layers and corresponding CBL will be pruned : [47, 48, 49, 53, 54, 55, 50, 51, 52, 44, 45, 46, 38, 39, 40, 13, 14, 15, 66, 67, 68, 72, 73, 74, 63, 64, 65, 41, 42, 43, 69, 70, 71, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "CBL_idx, Conv_idx, shortcut_idx = parse_module_defs4(model.module_defs)\n",
    "\n",
    "bn_weights = gather_bn_weights(model.module_list, shortcut_idx)\n",
    "sorted_bn = torch.sort(bn_weights)[0]\n",
    "\n",
    "bn_mean = torch.zeros(len(shortcut_idx))\n",
    "for i, idx in enumerate(shortcut_idx):\n",
    "    bn_mean[i] = model.module_list[idx][1].weight.data.abs().mean().clone()\n",
    "_, sorted_index_thre = torch.sort(bn_mean)\n",
    "\n",
    "\n",
    "prune_shortcuts = torch.tensor(shortcut_idx)[[sorted_index_thre[:opt.shortcuts]]]\n",
    "prune_shortcuts = [int(x) for x in prune_shortcuts]\n",
    "\n",
    "index_all = list(range(len(model.module_defs)))\n",
    "index_prune = []\n",
    "for idx in prune_shortcuts:\n",
    "    index_prune.extend([idx - 1, idx, idx + 1])\n",
    "index_remain = [idx for idx in index_all if idx not in index_prune]\n",
    "\n",
    "print('These shortcut layers and corresponding CBL will be pruned :', index_prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_filters_mask(model, CBL_idx, prune_shortcuts):\n",
    "\n",
    "    filters_mask = []\n",
    "    for idx in CBL_idx:\n",
    "        bn_module = model.module_list[idx][1]\n",
    "        mask = np.ones(bn_module.weight.data.shape[0], dtype='float32')\n",
    "        filters_mask.append(mask.copy())\n",
    "    CBLidx2mask = {idx: mask for idx, mask in zip(CBL_idx, filters_mask)}\n",
    "    for idx in prune_shortcuts:\n",
    "        for i in [idx, idx - 1]:\n",
    "            bn_module = model.module_list[i][1]\n",
    "            mask = np.zeros(bn_module.weight.data.shape[0], dtype='float32')\n",
    "            CBLidx2mask[i] = mask.copy()\n",
    "    return CBLidx2mask\n",
    "\n",
    "CBLidx2mask = obtain_filters_mask(model, CBL_idx, prune_shortcuts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input = torch.rand((1, 3, img_size, img_size)).to(device)\n",
    "pruned_model = prune_model_keep_size2(model, CBL_idx, CBL_idx, CBLidx2mask)\n",
    "\n",
    "def obtain_avg_forward_time(input, model, repeat=200):\n",
    "\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(repeat):\n",
    "            output = model(input)\n",
    "    avg_infer_time = (time.time() - start) / repeat\n",
    "\n",
    "    return avg_infer_time, output\n",
    "\n",
    "pruned_forward_time, pruned_output = obtain_avg_forward_time(random_input, pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 182 layers, 2.32107e+06 parameters, 2.32107e+06 gradients\n"
     ]
    }
   ],
   "source": [
    "compact_module_defs = deepcopy(model.module_defs)\n",
    "for j, module_def in enumerate(compact_module_defs):    \n",
    "    if module_def['type'] == 'route':\n",
    "        from_layers = [int(s) for s in module_def['layers']]\n",
    "        tmp = []\n",
    "        for index in range(len(from_layers)):\n",
    "            count = 0\n",
    "            if from_layers[index]>0:\n",
    "                for i in index_prune:\n",
    "                    if i<= from_layers[index]:\n",
    "                        count += 1\n",
    "                from_layers[index] = from_layers[index] - count\n",
    "            else:\n",
    "                for i in index_prune:\n",
    "                    if i > j + from_layers[index] and i < j:\n",
    "                        count += 1\n",
    "                from_layers[index] = from_layers[index] + count\n",
    "                \n",
    "        module_def['layers'] = from_layers\n",
    "\n",
    "\n",
    "compact_module_defs = [compact_module_defs[i] for i in index_remain]\n",
    "compact_model = Darknet([model.hyperparams.copy()] + compact_module_defs, (img_size, img_size)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# model_defs = {idx: mask for idx, mask in enumerate(compact_model.module_defs)}\n",
    "# model_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(index_remain):\n",
    "    compact_model.module_list[i] = pruned_model.module_list[index]\n",
    "    \n",
    "for j, module in enumerate(compact_model.module_list): \n",
    "    name = module.__class__.__name__\n",
    "    if name in ['FeatureConcat']:  # sum, concat\n",
    "        module.layers = compact_model.module_defs[j]['layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for i, model_def in enumerate(compact_model.module_defs):\n",
    "#     if model_def['type'] == 'route':\n",
    "#         from_layers = model_def['layers']\n",
    "#         print((i,from_layers))\n",
    "\n",
    "# for i, module in enumerate(compact_model.module_list):\n",
    "#     name = module.__class__.__name__\n",
    "#     if name in ['WeightedFeatureFusion', 'FeatureConcat']:  # sum, concat\n",
    "#         print((i,module.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching labels /home/tione/notebook/seu/VisDrone2019-DET-val/labels.txt (548 found, 0 missing, 0 empty, 0 duplicate, for 548 images): 100%|██████████| 548/548 [00:00<00:00, 3353.42it/s]\n",
      "               Class    Images   Targets         P         R   mAP@0.5        F1: 100%|██████████| 9/9 [01:40<00:00, 11.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       548  3.88e+04     0.386     0.474     0.404     0.423\n"
     ]
    }
   ],
   "source": [
    "random_input = torch.rand((1, 3, img_size, img_size)).to(device)\n",
    "\n",
    "def obtain_avg_forward_time(input, model, repeat=200):\n",
    "\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(repeat):\n",
    "            output = model(input)\n",
    "    avg_infer_time = (time.time() - start) / repeat\n",
    "\n",
    "    return avg_infer_time, output\n",
    "\n",
    "\n",
    "compact_forward_time, compact_output = obtain_avg_forward_time(random_input, compact_model)\n",
    "\n",
    "\n",
    "# 在测试集上测试剪枝后的模型, 并统计模型的参数数量\n",
    "with torch.no_grad():\n",
    "    compact_model_metric = eval_model(compact_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n",
      "| Metric     | Before   | After    |\n",
      "+------------+----------+----------+\n",
      "| mAP        | 0.420999 | 0.404273 |\n",
      "| Parameters | 4972797  | 2321067  |\n",
      "| Inference  | 0.0113   | 0.0093   |\n",
      "+------------+----------+----------+\n",
      "Config file has been saved: cfg/prune_12_shortcut_prune_0.8_keep_0.01_dense_yolov3_4.cfg\n",
      "Compact model has been saved: weights/prune_12_shortcut_prune_0.8_keep_0.01_best.weights\n"
     ]
    }
   ],
   "source": [
    "compact_nparameters = obtain_num_parameters(compact_model)\n",
    "# 比较剪枝前后参数数量的变化、指标性能的变化\n",
    "metric_table = [\n",
    "    [\"Metric\", \"Before\", \"After\"],\n",
    "    [\"mAP\", f'{origin_model_metric[0][2]:.6f}', f'{compact_model_metric[0][2]:.6f}'],\n",
    "    [\"Parameters\", f\"{origin_nparameters}\", f\"{compact_nparameters}\"],\n",
    "    [\"Inference\", f'{pruned_forward_time:.4f}', f'{compact_forward_time:.4f}']\n",
    "]\n",
    "print(AsciiTable(metric_table).table)\n",
    "\n",
    "\n",
    "# 生成剪枝后的cfg文件并保存模型\n",
    "pruned_cfg_name = opt.cfg.replace('/', f'/prune_{opt.shortcuts}_shortcut_')\n",
    "pruned_cfg_file = write_cfg(pruned_cfg_name, [model.hyperparams.copy()] + compact_module_defs)\n",
    "print(f'Config file has been saved: {pruned_cfg_file}')\n",
    "\n",
    "compact_model_name = opt.weights.replace('/', f'/prune_{opt.shortcuts}_shortcut_')\n",
    "if compact_model_name.endswith('.pt'):\n",
    "    compact_model_name = compact_model_name.replace('.pt', '.weights')\n",
    "\n",
    "save_weights(compact_model, path=compact_model_name)\n",
    "print(f'Compact model has been saved: {compact_model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
