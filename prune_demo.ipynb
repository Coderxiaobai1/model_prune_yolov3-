{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils.utils import *\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from test import test\n",
    "from terminaltables import AsciiTable\n",
    "import time\n",
    "from utils.prune_utils import *\n",
    "import argparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 254 layers, 6.19432e+07 parameters, 6.19432e+07 gradients\n",
      "Model Summary: 194 layers, 2.42303e+06 parameters, 2.42303e+06 gradients\n",
      "\n",
      "loaded weights from  weights/sparse_421map_best.pt\n",
      "\n",
      "loaded weights from  weights/last.pt\n"
     ]
    }
   ],
   "source": [
    "img_size = 800\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg1 = 'cfg/dense_yolov3_4.cfg'\n",
    "cfg2 = 'cfg/prune_10_shortcut_prune_0.8_keep_0.01_dense_yolov3_4.cfg'\n",
    "\n",
    "weights1 = 'weights/sparse_421map_best.pt'\n",
    "weights2 = 'weights/last.pt'\n",
    "\n",
    "model1 = Darknet(cfg1, (img_size, img_size)).to(device)\n",
    "model2 = Darknet(cfg2, (img_size, img_size)).to(device)\n",
    "\n",
    "\n",
    "        \n",
    "if weights1.endswith(\".pt\"):\n",
    "    model1.load_state_dict(torch.load(weights1, map_location=device)['model'])\n",
    "else:\n",
    "    _ = load_darknet_weights(model1, weights1)\n",
    "print('\\nloaded weights from ',weights1)\n",
    "\n",
    "if weights2.endswith(\".pt\"):\n",
    "    model2.load_state_dict(torch.load(weights2, map_location=device)['model'])\n",
    "else:\n",
    "    _ = load_darknet_weights(model2, weights2)\n",
    "print('\\nloaded weights from ',weights2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching labels /home/tione/notebook/seu/VisDrone2019-DET-val/labels.txt (548 found, 0 missing, 0 empty, 0 duplicate, for 548 images): 100%|██████████| 548/548 [00:00<00:00, 3370.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "let's test the original model first:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "               Class    Images   Targets         P         R   mAP@0.5        F1: 100%|██████████| 9/9 [01:46<00:00, 11.80s/it]\n",
      "Caching labels /home/tione/notebook/seu/VisDrone2019-DET-val/labels.txt (548 found, 0 missing, 0 empty, 0 duplicate, for 548 images): 100%|██████████| 548/548 [00:00<00:00, 3361.14it/s]\n",
      "               Class    Images   Targets         P         R   mAP@0.5        F1:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       548  3.88e+04     0.398     0.484     0.422     0.435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               Class    Images   Targets         P         R   mAP@0.5        F1: 100%|██████████| 9/9 [01:35<00:00, 10.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 all       548  3.88e+04     0.439     0.524      0.46     0.477\n"
     ]
    }
   ],
   "source": [
    "random_input = torch.rand((1, 3, img_size, img_size)).to(device)\n",
    "\n",
    "def obtain_avg_forward_time(input, model, repeat=200):\n",
    "\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(repeat):\n",
    "            output = model(input)\n",
    "    avg_infer_time = (time.time() - start) / repeat\n",
    "\n",
    "    return avg_infer_time, output\n",
    "\n",
    "model1_forward_time, _ = obtain_avg_forward_time(random_input, model1)\n",
    "model2_forward_time, _ = obtain_avg_forward_time(random_input, model2)\n",
    "\n",
    "\n",
    "\n",
    "eval_model = lambda model,cfg:test(model = model, cfg= cfg, data='data/visdrone.data',batch_size=64, imgsz=img_size,is_training = False)\n",
    "obtain_num_parameters = lambda model:sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(\"\\nlet's test the original model first:\")\n",
    "with torch.no_grad():\n",
    "    origin_model_metric1 = eval_model(model1,cfg1)\n",
    "    origin_model_metric2 = eval_model(model2,cfg2)\n",
    "origin_nparameters1 = obtain_num_parameters(model1)\n",
    "origin_nparameters2 = obtain_num_parameters(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+-------------+\n",
      "| Metric     | Before Prune | After Prune |\n",
      "+------------+--------------+-------------+\n",
      "| mAP        | 0.421684     | 0.460239    |\n",
      "| Parameters | 61943156     | 2423033     |\n",
      "| Inference  | 0.025589     | 0.009889    |\n",
      "+------------+--------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "metric_table = [\n",
    "    [\"Metric\", \"Before Prune\", \"After Prune\"],\n",
    "    [\"mAP\", f'{origin_model_metric1[0][2]:.6f}', f'{origin_model_metric2[0][2]:.6f}'],\n",
    "    [\"Parameters\", f\"{origin_nparameters1}\", f\"{origin_nparameters2}\"],\n",
    "    [\"Inference time\", f'{model1_forward_time:.6f}', f'{model2_forward_time:.6f}']\n",
    "]\n",
    "print(AsciiTable(metric_table).table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
